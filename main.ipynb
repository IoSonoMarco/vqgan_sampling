{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cc13f1ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"KMP_DUPLICATE_LIB_OK\"] = \"TRUE\"\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from pathlib import Path\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from lib.data import (\n",
    "    ImageTokenDataset,\n",
    "    ImageTokenDatasetClassLabel,\n",
    "    ImageTokenDatasetSemanticLabel\n",
    ")\n",
    "from lib.models import (\n",
    "    ConditionalTransformerDecoderConfig, ConditionalTransformerDecoder,\n",
    "    VanillaTransformerDecoderConfig, VanillaTransformerDecoder\n",
    ")\n",
    "from lib.training import (\n",
    "    ConditionalTransformerTrainer,\n",
    "    UnconditionalTransformerTrainer\n",
    ")\n",
    "\n",
    "device = torch.device(\"cuda:0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0e808f16",
   "metadata": {},
   "outputs": [],
   "source": [
    "ckpt_root = dict(\n",
    "    cond_l = \"./log/run_1/ckpt_e600_condl.pt\",\n",
    "    cond_s = \"./log/run_1/ckpt_e600_conds.pt\",\n",
    "    unc = \"./log/run_1/ckpt_e600_unc.pt\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51b7e187",
   "metadata": {},
   "source": [
    "#### Unconditional Token Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3f639954",
   "metadata": {},
   "outputs": [],
   "source": [
    "ckpt = torch.load(ckpt_root[\"unc\"], weights_only=False, map_location=\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "020dd893",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#params: 38878720\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config = VanillaTransformerDecoderConfig(**ckpt[\"model_config\"])\n",
    "model = VanillaTransformerDecoder(config)\n",
    "model.to(device)\n",
    "model.load_state_dict(ckpt[\"model_state_dict\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f3a9411b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [02:08<00:00,  2.57s/it]\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    model.eval()\n",
    "\n",
    "    sampled_images = []\n",
    "    for _ in tqdm(range(50)):\n",
    "        x = model.sos_token_id[None]\n",
    "        for _ in range(256):\n",
    "            pred = model.predict_next_token(x, 10)\n",
    "            x = torch.cat([x, torch.tensor(pred, device=device)[None]])\n",
    "        sampled_images.append(x[1:].cpu().numpy())\n",
    "    sampled_images = np.vstack(sampled_images)\n",
    "    sampled_images.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "86815c96",
   "metadata": {},
   "outputs": [],
   "source": [
    "savepath = Path(\"C:/Users/marco/Desktop/projects/taming-transformer/taming-transformers/sampled_images/unc/sample.npy\")\n",
    "savepath.parent.mkdir(parents=True, exist_ok=True)\n",
    "np.save(savepath, sampled_images)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57d70ea9",
   "metadata": {},
   "source": [
    "#### Label-Conditional Token Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "75c24c84",
   "metadata": {},
   "outputs": [],
   "source": [
    "ckpt = torch.load(ckpt_root[\"cond_l\"], weights_only=False, map_location=\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c742801c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#params: 43112448\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config = ConditionalTransformerDecoderConfig(**ckpt[\"model_config\"])\n",
    "model = ConditionalTransformerDecoder(config)\n",
    "model.to(device)\n",
    "model.load_state_dict(ckpt[\"model_state_dict\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "78746ede",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [00:13<00:00,  2.64s/it]\n",
      "100%|██████████| 5/5 [00:13<00:00,  2.64s/it]\n",
      "100%|██████████| 5/5 [00:13<00:00,  2.64s/it]\n",
      "100%|██████████| 5/5 [00:13<00:00,  2.66s/it]\n",
      "100%|██████████| 5/5 [00:13<00:00,  2.65s/it]\n",
      "100%|██████████| 5/5 [00:13<00:00,  2.66s/it]\n",
      "100%|██████████| 5/5 [00:13<00:00,  2.66s/it]\n",
      "100%|██████████| 5/5 [00:13<00:00,  2.64s/it]\n",
      "100%|██████████| 5/5 [00:13<00:00,  2.64s/it]\n",
      "100%|██████████| 5/5 [00:13<00:00,  2.67s/it]\n"
     ]
    }
   ],
   "source": [
    "target_class_labels = np.random.choice(config.n_classes, size=10, replace=False)\n",
    "\n",
    "with torch.no_grad():\n",
    "    model.eval()\n",
    "\n",
    "    sampled_images = {i:[] for i in target_class_labels}\n",
    "\n",
    "    for target_class_id in target_class_labels:\n",
    "\n",
    "        prompt = model.class_prompt_embedding(\n",
    "            torch.tensor(target_class_id, device=device)[None]\n",
    "        ).view(config.class_prompt_length, -1)\n",
    "\n",
    "        for _ in tqdm(range(5)):\n",
    "            x = torch.tensor([], device=device).long()\n",
    "            for _ in range(256):\n",
    "                pred = model.predict_next_token(prompt, x, 10)\n",
    "                x = torch.cat([x, torch.tensor(pred, device=device)[None]])\n",
    "            sampled_images[target_class_id].append(x.cpu().numpy())\n",
    "        sampled_images[target_class_id] = np.vstack(sampled_images[target_class_id])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "06cfae83",
   "metadata": {},
   "outputs": [],
   "source": [
    "savepath = Path(\"C:/Users/marco/Desktop/projects/taming-transformer/taming-transformers/sampled_images/cond_l/sample.npy\")\n",
    "savepath.parent.mkdir(parents=True, exist_ok=True)\n",
    "sampled_images = {int(k):v.tolist() for k,v in sampled_images.items()}\n",
    "np.save(savepath, sampled_images)\n",
    "\n",
    "savepath = Path(\"C:/Users/marco/Desktop/projects/taming-transformer/taming-transformers/sampled_images/cond_l/sample.pickle\")\n",
    "pickle.dump(sampled_images, open(savepath, \"wb\"), protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e9b748f",
   "metadata": {},
   "source": [
    "#### Semantic-Conditional Token Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d91574a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "ckpt = torch.load(ckpt_root[\"cond_s\"], weights_only=False, map_location=\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7fe938da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#params: 38918144\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config = ConditionalTransformerDecoderConfig(**ckpt[\"model_config\"])\n",
    "model = ConditionalTransformerDecoder(config)\n",
    "model.to(device)\n",
    "model.load_state_dict(ckpt[\"model_state_dict\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0badd4dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:26<00:00,  2.62s/it]\n",
      "100%|██████████| 10/10 [00:26<00:00,  2.61s/it]\n",
      "100%|██████████| 10/10 [00:26<00:00,  2.62s/it]\n",
      "100%|██████████| 10/10 [00:26<00:00,  2.63s/it]\n",
      "100%|██████████| 10/10 [00:26<00:00,  2.62s/it]\n"
     ]
    }
   ],
   "source": [
    "target_class_labels = np.random.choice(config.n_classes, size=5, replace=False)\n",
    "\n",
    "with torch.no_grad():\n",
    "    model.eval()\n",
    "\n",
    "    sampled_images = {i:[] for i in target_class_labels}\n",
    "\n",
    "    for target_class_id in target_class_labels:\n",
    "\n",
    "        prompt = model.class_prompt_embedding(\n",
    "            torch.tensor(target_class_id, device=device)[None]\n",
    "        ).view(config.class_prompt_length, -1)\n",
    "\n",
    "        for _ in tqdm(range(10)):\n",
    "            x = torch.tensor([], device=device).long()\n",
    "            for _ in range(256):\n",
    "                pred = model.predict_next_token(prompt, x, 10)\n",
    "                x = torch.cat([x, torch.tensor(pred, device=device)[None]])\n",
    "            sampled_images[target_class_id].append(x.cpu().numpy())\n",
    "        sampled_images[target_class_id] = np.vstack(sampled_images[target_class_id])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7e7bcee7",
   "metadata": {},
   "outputs": [],
   "source": [
    "savepath = Path(\"C:/Users/marco/Desktop/projects/taming-transformer/taming-transformers/sampled_images/cond_s/sample.npy\")\n",
    "savepath.parent.mkdir(parents=True, exist_ok=True)\n",
    "sampled_images = {int(k):v.tolist() for k,v in sampled_images.items()}\n",
    "np.save(savepath, sampled_images)\n",
    "\n",
    "savepath = Path(\"C:/Users/marco/Desktop/projects/taming-transformer/taming-transformers/sampled_images/cond_s/sample.pickle\")\n",
    "pickle.dump(sampled_images, open(savepath, \"wb\"), protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d1d92bd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "259dfbf1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "neuro",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
